feature,importance
qwen25_math_scores,0.16320401255317296
judge_meta-llama-3.1-405b-instruct-turbo_verdicts,0.10808823886471712
judge_gpt-4o_verdicts,0.08903688202145249
judge_llama-3.1-nemotron-70b-instruct-hf_verdicts,0.08378852972509586
judge_llama-3.3-70b-instruct-turbo_verdicts,0.06420157971826683
skyworks_scores,0.04245199009224596
judge_qwen2-72b-instruct_verdicts,0.038604284187369484
qrm_scores,0.03085639595460217
eurus_prm_scores,0.02992334260880013
judge_qwen2.5-72b-instruct-turbo_verdicts,0.026281744891236664
internlm2_scores,0.02451473628326562
urm_scores,0.023520082700447476
eurus_prm2_scores,0.020689375551405677
grm_llama32_scores,0.01887255277650451
offset_bias_scores,0.017943615316338254
internlm_scores,0.01701289050844864
armor_rm_coherence,0.016482949561475092
qrm_gemma_complexity,0.015379517488279109
armor_rm_score,0.01415602274904496
skywork_gemma_scores,0.013871053047706024
grm_scores,0.012952674659202615
qrm_gemma_coherence,0.011773666234297665
armor_rm_correctness,0.011725946381158182
grm_gemma_scores,0.011681798177162594
armor_rm_helpfulness,0.011497740328241265
armor_rm_verbosity,0.011435463209298608
qrm_gemma_scores,0.01137623330599355
armor_rm_complexity,0.011360276212888433
qrm_gemma_verbosity,0.010988826615368779
gpm_scores,0.010120725529113538
qrm_gemma_helpfulness,0.009229403748398278
qrm_gemma_correctness,0.009008020638805613
judge_qwq-32b-preview_verdicts,0.007486768265758774
judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts,0.0004826600944372404
