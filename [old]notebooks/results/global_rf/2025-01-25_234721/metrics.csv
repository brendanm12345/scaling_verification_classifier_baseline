selection_accuracy,selection_precision,selection_recall,selection_tp,selection_tn,selection_fp,selection_fn,generation_accuracy,feature_thresholds,selection_f1,train_percentage
0.62,0.62,0.6262626262626263,62,1,38,37,0.7407507507507507,"{'internlm_scores': np.float64(0.5610409080982208), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.7345903813838959), 'qrm_scores': np.float64(0.816645011305809), 'gpm_scores': np.float64(0.6660734415054321), 'judge_gpt-4o_verdicts': np.float64(0.5), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'grm_gemma_scores': np.float64(0.7830982953310013), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.7200915813446045), 'urm_scores': np.float64(0.7811333239078522), 'grm_scores': np.float64(0.6143136471509933), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'offset_bias_scores': np.float64(0.8042662143707275), 'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5)}",0.6231155778894472,0.001
0.63,0.63,0.6363636363636364,63,1,37,36,0.7766868686868686,"{'internlm_scores': np.float64(0.5856590270996094), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'grm_scores': np.float64(0.6414212584495544), 'grm_gemma_scores': np.float64(0.7173050940036774), 'qrm_scores': np.float64(0.7518562972545624), 'offset_bias_scores': np.float64(0.7900174260139465), 'gpm_scores': np.float64(0.6444178819656372), 'urm_scores': np.float64(0.7123889029026031), 'grm_llama32_scores': np.float64(0.7131018936634064), 'judge_gpt-4o_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.7241359949111938), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5)}",0.6331658291457286,0.01
0.65,0.65,0.6632653061224489,65,2,35,33,0.7945111111111111,"{'internlm_scores': np.float64(0.5770158469676971), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.7164848148822784), 'gpm_scores': np.float64(0.6281408667564392), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.7019412368535995), 'offset_bias_scores': np.float64(0.7697674334049225), 'grm_scores': np.float64(0.6082748472690582), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'urm_scores': np.float64(0.6800908893346786), 'qrm_scores': np.float64(0.7430640757083893), 'grm_gemma_scores': np.float64(0.6993381679058075), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_gpt-4o_verdicts': np.float64(0.5), 'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5), 'judge_qwq-32b-preview_verdicts': np.float64(0.5)}",0.6565656565656566,0.1
0.65,0.65,0.6632653061224489,65,2,35,33,0.7978428571428572,"{'internlm_scores': np.float64(0.571126714348793), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.7086741775274277), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'grm_gemma_scores': np.float64(0.6944662183523178), 'grm_llama32_scores': np.float64(0.6914289891719818), 'offset_bias_scores': np.float64(0.7521088421344757), 'grm_scores': np.float64(0.605427473783493), 'urm_scores': np.float64(0.6736802756786346), 'qrm_scores': np.float64(0.7424675822257996), 'gpm_scores': np.float64(0.6161803603172302), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_gpt-4o_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5), 'judge_qwq-32b-preview_verdicts': np.float64(0.5)}",0.6565656565656566,0.3
0.68,0.68,0.6938775510204082,68,2,32,30,0.7982,"{'internlm_scores': np.float64(0.5539487600326538), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.6949768513441086), 'qrm_scores': np.float64(0.7360959202051163), 'grm_gemma_scores': np.float64(0.6914969384670258), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'offset_bias_scores': np.float64(0.7404470592737198), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.6059397459030151), 'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'grm_scores': np.float64(0.6038504540920258), 'urm_scores': np.float64(0.6619961559772491), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_gpt-4o_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.6897246837615967), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5), 'judge_qwq-32b-preview_verdicts': np.float64(0.5)}",0.686868686868687,0.5
0.68,0.68,0.7010309278350515,68,3,32,29,0.8000666666666667,"{'internlm_scores': np.float64(0.5507000535726547), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'qrm_scores': np.float64(0.7255383282899857), 'urm_scores': np.float64(0.6647884547710419), 'gpm_scores': np.float64(0.6091411709785461), 'grm_llama32_scores': np.float64(0.6784742772579193), 'offset_bias_scores': np.float64(0.7332878708839417), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.6885001063346863), 'grm_scores': np.float64(0.5894011557102203), 'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'grm_gemma_scores': np.float64(0.6822602152824402), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_gpt-4o_verdicts': np.float64(0.5), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5), 'judge_qwq-32b-preview_verdicts': np.float64(0.5)}",0.6903553299492386,0.7
