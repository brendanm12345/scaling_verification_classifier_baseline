{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hazyresearch/MATH_with_LM_Judges_and_Reward_Model_Results_V2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 34 columns):\n",
      " #   Column                                             Non-Null Count  Dtype \n",
      "---  ------                                             --------------  ----- \n",
      " 0   internlm_scores                                    100 non-null    object\n",
      " 1   armor_rm_score                                     100 non-null    object\n",
      " 2   gpm_scores                                         100 non-null    object\n",
      " 3   armor_rm_helpfulness                               100 non-null    object\n",
      " 4   offset_bias_scores                                 100 non-null    object\n",
      " 5   armor_rm_correctness                               100 non-null    object\n",
      " 6   grm_llama32_scores                                 100 non-null    object\n",
      " 7   problem                                            100 non-null    object\n",
      " 8   samples                                            100 non-null    object\n",
      " 9   extracted_answers                                  100 non-null    object\n",
      " 10  qrm_scores                                         100 non-null    object\n",
      " 11  type                                               100 non-null    object\n",
      " 12  grm_scores                                         100 non-null    object\n",
      " 13  urm_scores                                         100 non-null    object\n",
      " 14  answer_correct                                     100 non-null    object\n",
      " 15  instruction                                        100 non-null    object\n",
      " 16  level                                              100 non-null    object\n",
      " 17  skyworks_scores                                    100 non-null    object\n",
      " 18  solution                                           100 non-null    object\n",
      " 19  armor_rm_top_attributes                            100 non-null    object\n",
      " 20  armor_rm_verbosity                                 100 non-null    object\n",
      " 21  armor_rm_complexity                                100 non-null    object\n",
      " 22  armor_rm_coherence                                 100 non-null    object\n",
      " 23  grm_gemma_scores                                   100 non-null    object\n",
      " 24  judge_qwen2-72b-instruct_verdicts                  100 non-null    object\n",
      " 25  judge_qwen2.5-72b-instruct-turbo_verdicts          100 non-null    object\n",
      " 26  judge_qwq-32b-preview_verdicts                     100 non-null    object\n",
      " 27  judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts      100 non-null    object\n",
      " 28  judge_llama-3.1-nemotron-70b-instruct-hf_verdicts  100 non-null    object\n",
      " 29  judge_meta-llama-3.1-405b-instruct-turbo_verdicts  100 non-null    object\n",
      " 30  judge_gemma-2-27b-it_verdicts                      100 non-null    object\n",
      " 31  judge_claude-3-5-sonnet-latest_verdicts            100 non-null    object\n",
      " 32  judge_llama-3.3-70b-instruct-turbo_verdicts        100 non-null    object\n",
      " 33  judge_gpt-4o_verdicts                              100 non-null    object\n",
      "dtypes: object(34)\n",
      "memory usage: 26.7+ KB\n",
      "None\n",
      "\n",
      "hazyresearch/AIMO_GPT-4o-mini_with_LM_Judges_and_RMs_v2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90 entries, 0 to 89\n",
      "Data columns (total 55 columns):\n",
      " #   Column                                             Non-Null Count  Dtype \n",
      "---  ------                                             --------------  ----- \n",
      " 0   armor_rm_coherence                                 90 non-null     object\n",
      " 1   armor_rm_complexity                                90 non-null     object\n",
      " 2   armor_rm_correctness                               90 non-null     object\n",
      " 3   armor_rm_helpfulness                               90 non-null     object\n",
      " 4   armor_rm_score                                     90 non-null     object\n",
      " 5   armor_rm_top_attributes                            90 non-null     object\n",
      " 6   armor_rm_verbosity                                 90 non-null     object\n",
      " 7   gpm_scores                                         90 non-null     object\n",
      " 8   grm_gemma_scores                                   90 non-null     object\n",
      " 9   grm_llama32_scores                                 90 non-null     object\n",
      " 10  grm_scores                                         90 non-null     object\n",
      " 11  instruction                                        90 non-null     object\n",
      " 12  internlm_scores                                    90 non-null     object\n",
      " 13  judge_claude-3-5-sonnet-latest_verdicts            90 non-null     object\n",
      " 14  judge_gemma-2-27b-it_verdicts                      90 non-null     object\n",
      " 15  judge_gpt-4o_verdicts                              90 non-null     object\n",
      " 16  judge_llama-3.1-nemotron-70b-instruct-hf_verdicts  90 non-null     object\n",
      " 17  judge_llama-3.3-70b-instruct-turbo_verdicts        90 non-null     object\n",
      " 18  judge_meta-llama-3.1-405b-instruct-turbo_verdicts  90 non-null     object\n",
      " 19  judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts      90 non-null     object\n",
      " 20  judge_qwen2-72b-instruct_verdicts                  90 non-null     object\n",
      " 21  judge_qwen2.5-72b-instruct-turbo_verdicts          90 non-null     object\n",
      " 22  judge_qwq-32b-preview_verdicts                     90 non-null     object\n",
      " 23  level                                              90 non-null     object\n",
      " 24  offset_bias_scores                                 90 non-null     object\n",
      " 25  problem                                            90 non-null     object\n",
      " 26  qrm_scores                                         90 non-null     object\n",
      " 27  samples                                            90 non-null     object\n",
      " 28  skyworks_scores                                    90 non-null     object\n",
      " 29  solution                                           90 non-null     object\n",
      " 30  type                                               90 non-null     object\n",
      " 31  urm_scores                                         90 non-null     object\n",
      " 32  answer_correct                                     90 non-null     object\n",
      " 33  skywork_gemma_scores                               90 non-null     object\n",
      " 34  internlm2_scores                                   90 non-null     object\n",
      " 35  qwen25_math_scores                                 90 non-null     object\n",
      " 36  prm_step_rewards                                   90 non-null     object\n",
      " 37  prm_num_steps                                      90 non-null     object\n",
      " 38  prm_mean_rewards                                   90 non-null     object\n",
      " 39  prm_min_rewards                                    90 non-null     object\n",
      " 40  prm_max_rewards                                    90 non-null     object\n",
      " 41  skywork_o1_step_rewards                            90 non-null     object\n",
      " 42  skywork_o1_overall_scores                          90 non-null     object\n",
      " 43  eurus_prm_scores                                   90 non-null     object\n",
      " 44  eurus_prm_step_rewards                             90 non-null     object\n",
      " 45  eurus_prm2_scores                                  90 non-null     object\n",
      " 46  eurus_prm2_step_rewards                            90 non-null     object\n",
      " 47  qrm_gemma_scores                                   90 non-null     object\n",
      " 48  qrm_gemma_quantiles                                90 non-null     object\n",
      " 49  qrm_gemma_helpfulness                              90 non-null     object\n",
      " 50  qrm_gemma_correctness                              90 non-null     object\n",
      " 51  qrm_gemma_coherence                                90 non-null     object\n",
      " 52  qrm_gemma_complexity                               90 non-null     object\n",
      " 53  qrm_gemma_verbosity                                90 non-null     object\n",
      " 54  extracted_answers                                  90 non-null     object\n",
      "dtypes: object(55)\n",
      "memory usage: 38.8+ KB\n",
      "None\n",
      "\n",
      "hazyresearch/CodeContests_Llama_70B_with_LM_Judges_and_RMs_v1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                             Non-Null Count  Dtype \n",
      "---  ------                                             --------------  ----- \n",
      " 0   answer_correct                                     140 non-null    object\n",
      " 1   armor_rm_coherence                                 140 non-null    object\n",
      " 2   armor_rm_complexity                                140 non-null    object\n",
      " 3   armor_rm_correctness                               140 non-null    object\n",
      " 4   armor_rm_helpfulness                               140 non-null    object\n",
      " 5   armor_rm_score                                     140 non-null    object\n",
      " 6   armor_rm_top_attributes                            140 non-null    object\n",
      " 7   armor_rm_verbosity                                 140 non-null    object\n",
      " 8   extracted_answers                                  140 non-null    object\n",
      " 9   gpm_scores                                         140 non-null    object\n",
      " 10  grm_gemma_scores                                   140 non-null    object\n",
      " 11  grm_llama32_scores                                 140 non-null    object\n",
      " 12  grm_scores                                         140 non-null    object\n",
      " 13  instruction                                        140 non-null    object\n",
      " 14  internlm_scores                                    140 non-null    object\n",
      " 15  judge_claude-3-5-sonnet-latest_verdicts            140 non-null    object\n",
      " 16  judge_gemma-2-27b-it_verdicts                      140 non-null    object\n",
      " 17  judge_gpt-4o_verdicts                              140 non-null    object\n",
      " 18  judge_llama-3.1-nemotron-70b-instruct-hf_verdicts  140 non-null    object\n",
      " 19  judge_llama-3.3-70b-instruct-turbo_verdicts        140 non-null    object\n",
      " 20  judge_meta-llama-3.1-405b-instruct-turbo_verdicts  140 non-null    object\n",
      " 21  judge_mixtral-8x22b-instruct-v0.1_verdicts         140 non-null    object\n",
      " 22  judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts      140 non-null    object\n",
      " 23  judge_qwen2-72b-instruct_verdicts                  140 non-null    object\n",
      " 24  judge_qwen2.5-72b-instruct-turbo_verdicts          140 non-null    object\n",
      " 25  judge_qwq-32b-preview_verdicts                     140 non-null    object\n",
      " 26  judge_wizardlm-2-8x22b_verdicts                    140 non-null    object\n",
      " 27  level                                              140 non-null    object\n",
      " 28  offset_bias_scores                                 140 non-null    object\n",
      " 29  problem                                            140 non-null    object\n",
      " 30  qrm_scores                                         140 non-null    object\n",
      " 31  samples                                            140 non-null    object\n",
      " 32  skyworks_scores                                    140 non-null    object\n",
      " 33  solution                                           140 non-null    object\n",
      " 34  type                                               140 non-null    object\n",
      " 35  urm_scores                                         140 non-null    object\n",
      "dtypes: object(36)\n",
      "memory usage: 39.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ds = [\"hazyresearch/MATH_with_LM_Judges_and_Reward_Model_Results_V2\", \"hazyresearch/AIMO_GPT-4o-mini_with_LM_Judges_and_RMs_v2\", \"hazyresearch/CodeContests_Llama_70B_with_LM_Judges_and_RMs_v1\"]\n",
    "dataset = load_dataset(ds[0])\n",
    "data = dataset['data']\n",
    "problems_df = pd.DataFrame(data)\n",
    "print(f\"\\n{ds[0]}\")\n",
    "print(problems_df.info())\n",
    "dataset = load_dataset(ds[1])\n",
    "data = dataset['data']\n",
    "problems_df = pd.DataFrame(data)\n",
    "print(f\"\\n{ds[1]}\")\n",
    "print(problems_df.info())\n",
    "dataset = load_dataset(ds[2])\n",
    "data = dataset['data']\n",
    "problems_df = pd.DataFrame(data)\n",
    "print(f\"\\n{ds[2]}\")\n",
    "print(problems_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                             Non-Null Count  Dtype \n",
      "---  ------                                             --------------  ----- \n",
      " 0   answer_correct                                     140 non-null    object\n",
      " 1   armor_rm_coherence                                 140 non-null    object\n",
      " 2   armor_rm_complexity                                140 non-null    object\n",
      " 3   armor_rm_correctness                               140 non-null    object\n",
      " 4   armor_rm_helpfulness                               140 non-null    object\n",
      " 5   armor_rm_score                                     140 non-null    object\n",
      " 6   armor_rm_top_attributes                            140 non-null    object\n",
      " 7   armor_rm_verbosity                                 140 non-null    object\n",
      " 8   extracted_answers                                  140 non-null    object\n",
      " 9   gpm_scores                                         140 non-null    object\n",
      " 10  grm_gemma_scores                                   140 non-null    object\n",
      " 11  grm_llama32_scores                                 140 non-null    object\n",
      " 12  grm_scores                                         140 non-null    object\n",
      " 13  instruction                                        140 non-null    object\n",
      " 14  internlm_scores                                    140 non-null    object\n",
      " 15  judge_claude-3-5-sonnet-latest_verdicts            140 non-null    object\n",
      " 16  judge_gemma-2-27b-it_verdicts                      140 non-null    object\n",
      " 17  judge_gpt-4o_verdicts                              140 non-null    object\n",
      " 18  judge_llama-3.1-nemotron-70b-instruct-hf_verdicts  140 non-null    object\n",
      " 19  judge_llama-3.3-70b-instruct-turbo_verdicts        140 non-null    object\n",
      " 20  judge_meta-llama-3.1-405b-instruct-turbo_verdicts  140 non-null    object\n",
      " 21  judge_mixtral-8x22b-instruct-v0.1_verdicts         140 non-null    object\n",
      " 22  judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts      140 non-null    object\n",
      " 23  judge_qwen2-72b-instruct_verdicts                  140 non-null    object\n",
      " 24  judge_qwen2.5-72b-instruct-turbo_verdicts          140 non-null    object\n",
      " 25  judge_qwq-32b-preview_verdicts                     140 non-null    object\n",
      " 26  judge_wizardlm-2-8x22b_verdicts                    140 non-null    object\n",
      " 27  level                                              140 non-null    object\n",
      " 28  offset_bias_scores                                 140 non-null    object\n",
      " 29  problem                                            140 non-null    object\n",
      " 30  qrm_scores                                         140 non-null    object\n",
      " 31  samples                                            140 non-null    object\n",
      " 32  skyworks_scores                                    140 non-null    object\n",
      " 33  solution                                           140 non-null    object\n",
      " 34  type                                               140 non-null    object\n",
      " 35  urm_scores                                         140 non-null    object\n",
      "dtypes: object(36)\n",
      "memory usage: 39.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = dataset['data']\n",
    "problems_df = pd.DataFrame(data)\n",
    "problems_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few problems (basic info): \n",
      "                                         instruction  \\\n",
      "0  Q: Write python code to solve the following co...   \n",
      "1  Q: Write python code to solve the following co...   \n",
      "2  Q: Write python code to solve the following co...   \n",
      "3  Q: Write python code to solve the following co...   \n",
      "4  Q: Write python code to solve the following co...   \n",
      "\n",
      "                                   extracted_answers  \n",
      "0  [def count_segments(n, m, k, a):\\n    MOD = 10...  \n",
      "1  [from itertools import product\\n\\ns = input()\\...  \n",
      "2  [from collections import defaultdict\\n\\nMOD = ...  \n",
      "3  [def power(a, n, mod):\\n    res = 1\\n    while...  \n",
      "4  [def gcd(a, b):\\n    while b:\\n        a, b = ...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst few problems (basic info): \")\n",
    "df_simple = problems_df[['instruction', 'extracted_answers']].head()\n",
    "print(df_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1000\n",
      "1    1000\n",
      "2    1000\n",
      "3    1000\n",
      "4    1000\n",
      "Name: extracted_answers, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(problems_df['extracted_answers'].str.len().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hazyresearch/AIMO_GPT-4o-mini_with_LM_Judges_and_RMs_v2:\n",
    "Properties:\n",
    "    - 55 weak verifiers\n",
    "        - Judge\n",
    "        - RM:\n",
    "    - 90 problems\n",
    "    - 100 generations per problem\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hazyresearch/CodeContests_Llama_70B_with_LM_Judges_and_RMs_v1:\n",
    "Properties:\n",
    "    - 140 problems\n",
    "    - 36 weak verifiers\n",
    "        - Judge\n",
    "        - RM:\n",
    "    - 1000 generations per problem\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    data = dataset['data']\n",
    "\n",
    "    METADATA_COLUMNS = {\n",
    "        'problem', 'samples', 'solution', 'instruction', 'type', 'level', 'answer_correct', 'extracted_answers',\n",
    "        'problem_idx', 'solution_idx'\n",
    "    }\n",
    "    \n",
    "    # get first row (problem) to identify columns\n",
    "    first_item = data[0]\n",
    "    all_columns = set(first_item.keys())\n",
    "    \n",
    "    # First identify judge columns (they're special case with T/F values)\n",
    "    judge_columns = [col for col in all_columns \n",
    "                    if col.startswith('judge_') and col not in METADATA_COLUMNS]\n",
    "    \n",
    "    # Then identify numeric feature columns (excluding judge columns)\n",
    "    numeric_columns = [col for col in all_columns \n",
    "                      if col not in METADATA_COLUMNS and \n",
    "                      col not in judge_columns and\n",
    "                      isinstance(first_item[col][0], (int, float, np.number))]\n",
    "    \n",
    "    # 1st pass: collect all values for each verdict column\n",
    "    verdict_values = {col: [] for col in judge_columns}\n",
    "    for problem in data:\n",
    "        num_samples = len(problem['samples'])\n",
    "        for col in judge_columns:\n",
    "            # Only collect values if they match the number of samples\n",
    "            if len(problem[col]) == num_samples:\n",
    "                verdict_values[col].extend([v for v in problem[col] if v is not None])\n",
    "    \n",
    "    # calc mode for each verdict column with safety check\n",
    "    modes = {}\n",
    "    for col, values in verdict_values.items():\n",
    "        if not values:  # If all values were None\n",
    "            modes[col] = False  # default to False for empty columns\n",
    "        else:\n",
    "            modes[col] = max(set(values), key=values.count)\n",
    "    \n",
    "    # score cols are numeric columns that end with \"_score(s)\"\n",
    "    score_columns = [col for col in numeric_columns\n",
    "                    if col.endswith('_score') or col.endswith('_scores')]\n",
    "    \n",
    "    # other numerical columns (e.g. rewards, steps)\n",
    "    other_num_columns = [col for col in numeric_columns\n",
    "                        if col not in score_columns]\n",
    "    \n",
    "    data_rows = []\n",
    "    for idx in range(len(data)):\n",
    "        problem = data[idx]\n",
    "        num_samples = len(problem['samples'])\n",
    "        \n",
    "        # filter columns to only those that match the number of samples/generations\n",
    "        valid_judge_columns = [col for col in judge_columns if len(problem[col]) == num_samples]\n",
    "        valid_score_columns = [col for col in score_columns if len(problem[col]) == num_samples]\n",
    "        valid_other_columns = [col for col in other_num_columns if len(problem[col]) == num_samples]\n",
    "        \n",
    "        normalized_scores = normalize_scores(problem, valid_score_columns)\n",
    "        normalized_nums = normalize_scores(problem, valid_other_columns)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            if i < len(problem['answer_correct']):  # check if we have a corresponding answer\n",
    "                row = {\n",
    "                    'problem_idx': idx,\n",
    "                    'solution_idx': i,\n",
    "                    'is_correct': problem['answer_correct'][i],\n",
    "                    # Handle judge columns separately (no normalization)\n",
    "                    **{column: modes[column] if problem[column][i] is None\n",
    "                       else problem[column][i] for column in valid_judge_columns},\n",
    "                    # Normalize numeric columns\n",
    "                    **{column: normalized_scores[column][i] for column in valid_score_columns},\n",
    "                    **{column: normalized_nums[column][i] for column in valid_other_columns}\n",
    "                }\n",
    "                data_rows.append(row)\n",
    "            \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    final_feature_columns = [col for col in (judge_columns + numeric_columns) \n",
    "                            if col in df.columns]\n",
    "    \n",
    "    return df, final_feature_columns\n",
    "\n",
    "def normalize_scores(problem, columns):\n",
    "    \"\"\"Normalize scores within each problem\"\"\"\n",
    "    normalized = {}\n",
    "    for col in columns:\n",
    "        values = np.array(problem[col])\n",
    "        min_val = np.min(values)\n",
    "        max_val = np.max(values)\n",
    "        score_range = max_val - min_val\n",
    "        if score_range == 0:\n",
    "            normalized[col] = [0.5] * len(values)\n",
    "        else:\n",
    "            normalized[col] = (values - min_val) / score_range\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 28)\n",
      "Number of features: 25\n",
      "\n",
      "Feature columns: ['judge_qwen2-72b-instruct_verdicts', 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts', 'judge_claude-3-5-sonnet-latest_verdicts', 'judge_qwen2.5-72b-instruct-turbo_verdicts', 'judge_gemma-2-27b-it_verdicts', 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts', 'judge_llama-3.3-70b-instruct-turbo_verdicts', 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts', 'judge_gpt-4o_verdicts', 'judge_qwq-32b-preview_verdicts', 'armor_rm_verbosity', 'gpm_scores', 'grm_scores', 'internlm_scores', 'armor_rm_coherence', 'offset_bias_scores', 'grm_gemma_scores', 'grm_llama32_scores', 'skyworks_scores', 'armor_rm_complexity', 'armor_rm_score', 'urm_scores', 'armor_rm_correctness', 'qrm_scores', 'armor_rm_helpfulness']\n",
      "Dataset shape: (9000, 37)\n",
      "Number of features: 36\n",
      "\n",
      "Feature columns: ['judge_qwen2-72b-instruct_verdicts', 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts', 'judge_claude-3-5-sonnet-latest_verdicts', 'judge_qwen2.5-72b-instruct-turbo_verdicts', 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts', 'judge_llama-3.3-70b-instruct-turbo_verdicts', 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts', 'judge_gpt-4o_verdicts', 'judge_qwq-32b-preview_verdicts', 'judge_gemma-2-27b-it_verdicts', 'skywork_gemma_scores', 'armor_rm_verbosity', 'qrm_gemma_correctness', 'gpm_scores', 'qrm_gemma_complexity', 'qwen25_math_scores', 'qrm_gemma_verbosity', 'internlm2_scores', 'qrm_gemma_scores', 'grm_scores', 'internlm_scores', 'armor_rm_coherence', 'offset_bias_scores', 'qrm_gemma_coherence', 'grm_gemma_scores', 'grm_llama32_scores', 'skyworks_scores', 'qrm_gemma_helpfulness', 'armor_rm_complexity', 'armor_rm_score', 'urm_scores', 'armor_rm_correctness', 'qrm_scores', 'eurus_prm2_scores', 'eurus_prm_scores', 'armor_rm_helpfulness']\n",
      "Dataset shape: (140000, 30)\n",
      "Number of features: 27\n",
      "\n",
      "Feature columns: ['judge_qwen2-72b-instruct_verdicts', 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts', 'judge_mixtral-8x22b-instruct-v0.1_verdicts', 'judge_qwq-32b-preview_verdicts', 'judge_claude-3-5-sonnet-latest_verdicts', 'judge_qwen2.5-72b-instruct-turbo_verdicts', 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts', 'judge_llama-3.3-70b-instruct-turbo_verdicts', 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts', 'judge_gpt-4o_verdicts', 'judge_wizardlm-2-8x22b_verdicts', 'judge_gemma-2-27b-it_verdicts', 'armor_rm_verbosity', 'gpm_scores', 'grm_scores', 'internlm_scores', 'armor_rm_coherence', 'offset_bias_scores', 'grm_gemma_scores', 'grm_llama32_scores', 'skyworks_scores', 'armor_rm_complexity', 'armor_rm_score', 'urm_scores', 'armor_rm_correctness', 'qrm_scores', 'armor_rm_helpfulness']\n"
     ]
    }
   ],
   "source": [
    "ds = [\"hazyresearch/MATH_with_LM_Judges_and_Reward_Model_Results_V2\", \"hazyresearch/AIMO_GPT-4o-mini_with_LM_Judges_and_RMs_v2\", \"hazyresearch/CodeContests_Llama_70B_with_LM_Judges_and_RMs_v1\"]\n",
    "df, feature_columns = prepare_data(ds[0])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"\\nFeature columns: {feature_columns}\")\n",
    "\n",
    "df, feature_columns = prepare_data(ds[1])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"\\nFeature columns: {feature_columns}\")\n",
    "\n",
    "df, feature_columns = prepare_data(ds[2])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"\\nFeature columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
