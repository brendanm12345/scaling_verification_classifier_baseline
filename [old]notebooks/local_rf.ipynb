{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Tuple\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "from baseline.utils.setup import setup_experiment_dir\n",
    "exp_dir = setup_experiment_dir('local_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    data = dataset['data']\n",
    "\n",
    "    METADATA_COLUMNS = {\n",
    "        'problem', 'samples', 'solution', 'instruction', 'type', 'level', 'answer_correct', 'extracted_answers',\n",
    "        'problem_idx', 'solution_idx'\n",
    "    }\n",
    "    \n",
    "    # get first row (problem) to identify columns\n",
    "    first_item = data[0]\n",
    "    all_columns = set(first_item.keys())\n",
    "    \n",
    "    # First identify judge columns (they're special case with T/F values)\n",
    "    judge_columns = [col for col in all_columns \n",
    "                    if col.startswith('judge_') and col not in METADATA_COLUMNS]\n",
    "    \n",
    "    # Then identify numeric feature columns (excluding judge columns)\n",
    "    numeric_columns = [col for col in all_columns \n",
    "                      if col not in METADATA_COLUMNS and \n",
    "                      col not in judge_columns and\n",
    "                      isinstance(first_item[col][0], (int, float, np.number))]\n",
    "    \n",
    "    # 1st pass: collect all values for each verdict column\n",
    "    verdict_values = {col: [] for col in judge_columns}\n",
    "    for problem in data:\n",
    "        num_samples = len(problem['samples'])\n",
    "        for col in judge_columns:\n",
    "            # Only collect values if they match the number of samples\n",
    "            if len(problem[col]) == num_samples:\n",
    "                verdict_values[col].extend([v for v in problem[col] if v is not None])\n",
    "    \n",
    "    # calc mode for each verdict column with safety check\n",
    "    modes = {}\n",
    "    for col, values in verdict_values.items():\n",
    "        if not values:  # If all values were None\n",
    "            modes[col] = False  # default to False for empty columns\n",
    "        else:\n",
    "            modes[col] = max(set(values), key=values.count)\n",
    "    \n",
    "    # score cols are numeric columns that end with \"_score(s)\"\n",
    "    score_columns = [col for col in numeric_columns\n",
    "                    if col.endswith('_score') or col.endswith('_scores')]\n",
    "    \n",
    "    # other numerical columns (e.g. rewards, steps)\n",
    "    other_num_columns = [col for col in numeric_columns\n",
    "                        if col not in score_columns]\n",
    "    \n",
    "    data_rows = []\n",
    "    for idx in range(len(data)):\n",
    "        problem = data[idx]\n",
    "        num_samples = len(problem['samples'])\n",
    "        \n",
    "        # filter columns to only those that match the number of samples/generations\n",
    "        valid_judge_columns = [col for col in judge_columns if len(problem[col]) == num_samples]\n",
    "        valid_score_columns = [col for col in score_columns if len(problem[col]) == num_samples]\n",
    "        valid_other_columns = [col for col in other_num_columns if len(problem[col]) == num_samples]\n",
    "        \n",
    "        normalized_scores = normalize_scores(problem, valid_score_columns)\n",
    "        normalized_nums = normalize_scores(problem, valid_other_columns)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            if i < len(problem['answer_correct']):  # check if we have a corresponding answer\n",
    "                row = {\n",
    "                    'problem_idx': idx,\n",
    "                    'solution_idx': i,\n",
    "                    'is_correct': problem['answer_correct'][i],\n",
    "                    # Handle judge columns separately (no normalization)\n",
    "                    **{column: modes[column] if problem[column][i] is None\n",
    "                       else problem[column][i] for column in valid_judge_columns},\n",
    "                    # Normalize numeric columns\n",
    "                    **{column: normalized_scores[column][i] for column in valid_score_columns},\n",
    "                    **{column: normalized_nums[column][i] for column in valid_other_columns}\n",
    "                }\n",
    "                data_rows.append(row)\n",
    "            \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    final_feature_columns = [col for col in (judge_columns + numeric_columns) \n",
    "                            if col in df.columns]\n",
    "    \n",
    "    return df, final_feature_columns\n",
    "\n",
    "def normalize_scores(problem, columns):\n",
    "    \"\"\"Normalize scores within each problem\"\"\"\n",
    "    normalized = {}\n",
    "    for col in columns:\n",
    "        values = np.array(problem[col])\n",
    "        min_val = np.min(values)\n",
    "        max_val = np.max(values)\n",
    "        score_range = max_val - min_val\n",
    "        if score_range == 0:\n",
    "            normalized[col] = [0.5] * len(values)\n",
    "        else:\n",
    "            normalized[col] = (values - min_val) / score_range\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def get_feature_thresholds(rf_model, feature_columns) -> Dict:\n",
    "    \"\"\"Get median thresholds for each feature from the RF model\"\"\"\n",
    "    if rf_model is None:\n",
    "        return {}\n",
    "\n",
    "    feature_thresholds = {}\n",
    "    for tree in rf_model.estimators_:\n",
    "        for feature_idx, threshold in zip(tree.tree_.feature, tree.tree_.threshold):\n",
    "            if feature_idx >= 0:\n",
    "                feature_name = feature_columns[feature_idx]\n",
    "                if feature_name not in feature_thresholds:\n",
    "                    feature_thresholds[feature_name] = []\n",
    "                feature_thresholds[feature_name].append(threshold)\n",
    "\n",
    "    median_thresholds = {\n",
    "        feature: np.median(thresholds)\n",
    "        for feature, thresholds in feature_thresholds.items()\n",
    "    }\n",
    "\n",
    "    return median_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df_test, y_pred, y_pred_proba, rf_model, feature_columns):\n",
    "    \"\"\"Calculate metrics for a single problem's local RF model\"\"\"\n",
    "    # Generation accuracy\n",
    "    generation_accuracy = accuracy_score(df_test['is_correct'], y_pred)\n",
    "    \n",
    "    # Selection metrics\n",
    "    prob_proba = y_pred_proba\n",
    "    prob_labels = df_test['is_correct'].values\n",
    "    \n",
    "    # Select solution with highest probability\n",
    "    selected_idx = np.argmax(prob_proba)\n",
    "    \n",
    "    # Calculate base metrics\n",
    "    tp = 1 if prob_labels[selected_idx] else 0\n",
    "    fp = 1 if not prob_labels[selected_idx] else 0\n",
    "    fn = 1 if sum(prob_labels) > 0 and not prob_labels[selected_idx] else 0\n",
    "    tn = 1 if sum(prob_labels) == 0 and not prob_labels[selected_idx] else 0\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    selection_accuracy = tp  # Will be 1 or 0 for a single problem\n",
    "    selection_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    selection_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    selection_f1 = 2 * (selection_precision * selection_recall) / (\n",
    "        selection_precision + selection_recall) if (selection_precision + selection_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'problem_idx': df_test['problem_idx'].iloc[0],\n",
    "        'generation_accuracy': generation_accuracy,\n",
    "        'selection_accuracy': selection_accuracy,\n",
    "        'selection_precision': selection_precision,\n",
    "        'selection_recall': selection_recall,\n",
    "        'selection_f1': selection_f1,\n",
    "        'selection_tp': tp,\n",
    "        'selection_tn': tn,\n",
    "        'selection_fp': fp,\n",
    "        'selection_fn': fn,\n",
    "        'feature_thresholds': get_feature_thresholds(rf_model, feature_columns)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_problem_rf(problem_data: Tuple[pd.DataFrame, float, List[str]]) -> Dict:\n",
    "    \"\"\"Train and evaluate RF model for a single problem\"\"\"\n",
    "    df_prob, train_percentage, feature_columns = problem_data\n",
    "    \n",
    "    X = df_prob[feature_columns].copy()\n",
    "    y = df_prob['is_correct']\n",
    "    \n",
    "    n_positive = y.sum()\n",
    "    \n",
    "    # Case 1: insufficient positive examples - predict majority class\n",
    "    if n_positive <= 3:\n",
    "        majority_class = int(y.mean() >= 0.5)\n",
    "        y_pred = np.full(len(y), majority_class)\n",
    "        y_pred_proba = np.full(len(y), float(majority_class))\n",
    "        metrics = calculate_metrics(df_prob, y_pred, y_pred_proba, None, feature_columns)\n",
    "        metrics['insufficient_data'] = True\n",
    "        return metrics\n",
    "    \n",
    "    # Case 2: sufficient data for training\n",
    "    skf = StratifiedKFold(n_splits=max(2, int(np.ceil(1/train_percentage))),\n",
    "                         shuffle=True, random_state=42)\n",
    "    train_idx, test_idx = next(skf.split(X, y))\n",
    "    \n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "    df_test = df_prob.iloc[test_idx]\n",
    "    \n",
    "    # Fixed RF parameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = calculate_metrics(df_test, y_pred, y_pred_proba, rf, feature_columns)\n",
    "    metrics['insufficient_data'] = False\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics(problem_metrics: List[Dict]) -> Dict:\n",
    "    \"\"\"Aggregate metrics across all problems\"\"\"\n",
    "    return {\n",
    "        'generation_accuracy': np.mean([m['generation_accuracy'] for m in problem_metrics]),\n",
    "        'selection_accuracy': np.mean([m['selection_accuracy'] for m in problem_metrics]),\n",
    "        'selection_precision': np.mean([m['selection_precision'] for m in problem_metrics]),\n",
    "        'selection_recall': np.mean([m['selection_recall'] for m in problem_metrics]),\n",
    "        'selection_f1': np.mean([m['selection_f1'] for m in problem_metrics]),\n",
    "        'selection_tp': sum(m['selection_tp'] for m in problem_metrics),\n",
    "        'selection_tn': sum(m['selection_tn'] for m in problem_metrics),\n",
    "        'selection_fp': sum(m['selection_fp'] for m in problem_metrics),\n",
    "        'selection_fn': sum(m['selection_fn'] for m in problem_metrics),\n",
    "        'total_problems': len(problem_metrics),\n",
    "        'insufficient_data_problems': sum(1 for m in problem_metrics if m.get('insufficient_data', False))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_rfs(train_percentage: float):\n",
    "    \"\"\"Train and evaluate local RF models in parallel\"\"\"\n",
    "    print(f\"\\nTraining with {train_percentage*100}% of problems\")\n",
    "    \n",
    "    df, feature_columns = prepare_data(\"hazyresearch/CodeContests_Llama_70B_with_LM_Judges_and_RMs_v1\")\n",
    "    \n",
    "    problem_groups = [\n",
    "        (group, train_percentage, feature_columns)\n",
    "        for _, group in df.groupby('problem_idx')\n",
    "    ]\n",
    "    \n",
    "    print(\"Training models for each problem...\")\n",
    "    problem_metrics = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(train_problem_rf)(problem_data)\n",
    "        for problem_data in problem_groups\n",
    "    )\n",
    "    \n",
    "    aggregated_metrics = aggregate_metrics(problem_metrics)\n",
    "    \n",
    "    problem_metrics_df = pd.DataFrame(problem_metrics)\n",
    "    problem_metrics_df.to_csv(\n",
    "        os.path.join(exp_dir, 'per_problem', f'metrics_{train_percentage}.csv'),\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    return aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local RF training pipeline...\n",
      "\n",
      "Training with 10.0% of problems\n",
      "Training models for each problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    2.0s\n",
      "/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/brendanmclaughlin/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/sq/39g8ms_j5379qqpr7x5460r40000gn/T/ipykernel_60616/2205067836.py\", line 44, in train_problem_rf\nIndexError: index 1 is out of bounds for axis 1 with size 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_percentage \u001b[38;5;129;01min\u001b[39;00m train_percentages:\n\u001b[0;32m---> 38\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_local_rfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_percentage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_percentage\n\u001b[1;32m     40\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(metrics)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain_local_rfs\u001b[0;34m(train_percentage)\u001b[0m\n\u001b[1;32m      7\u001b[0m problem_groups \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     (group, train_percentage, feature_columns)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem_idx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining models for each problem...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m problem_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_problem_rf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproblem_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproblem_groups\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m aggregated_metrics \u001b[38;5;241m=\u001b[39m aggregate_metrics(problem_metrics)\n\u001b[1;32m     20\u001b[0m problem_metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(problem_metrics)\n",
      "File \u001b[0;32m~/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/agents/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "def plot_results(results_df):\n",
    "    \"\"\"Plot accuracy metrics\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Selection@1 plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(results_df['train_percentage'] * 100,\n",
    "             results_df['selection_accuracy'],\n",
    "             '-o', label='Local RF')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('Selection@1')\n",
    "    plt.title('Selection@1 vs Training Data Size')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    # Generation Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(results_df['train_percentage'] * 100,\n",
    "             results_df['generation_accuracy'],\n",
    "             '-o', label='Local RF')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('Generation Accuracy')\n",
    "    plt.title('Generation Accuracy vs Training Data Size')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, 'plots.png'))\n",
    "    plt.close()\n",
    "\n",
    "print(\"Starting local RF training pipeline...\")\n",
    "train_percentages = [0.1, 0.3, 0.5, 0.7]\n",
    "results = []\n",
    "\n",
    "for train_percentage in train_percentages:\n",
    "    metrics = train_local_rfs(train_percentage)\n",
    "    metrics['train_percentage'] = train_percentage\n",
    "    results.append(metrics)\n",
    "    \n",
    "    print(f\"\\nResults for {train_percentage*100}% training data:\")\n",
    "    print(f\"Selection accuracy: {metrics['selection_accuracy']:.3f}\")\n",
    "    print(f\"Generation accuracy: {metrics['generation_accuracy']:.3f}\")\n",
    "    print(f\"Selection F1: {metrics['selection_f1']:.3f}\")\n",
    "    print(f\"Insufficient data problems: {metrics['insufficient_data_problems']}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(exp_dir, 'metrics.csv'), index=False)\n",
    "\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
