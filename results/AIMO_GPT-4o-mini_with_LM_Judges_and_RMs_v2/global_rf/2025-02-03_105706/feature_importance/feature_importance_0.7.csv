feature,importance
qwen25_math_scores,0.16839905436704483
judge_meta-llama-3.1-405b-instruct-turbo_verdicts,0.11258980055488557
judge_llama-3.1-nemotron-70b-instruct-hf_verdicts,0.08198042604372781
judge_gpt-4o_verdicts,0.08151473020908244
judge_llama-3.3-70b-instruct-turbo_verdicts,0.05433973619264255
judge_qwen2-72b-instruct_verdicts,0.04517620529659754
skyworks_scores,0.04134270828926562
qrm_scores,0.028993691985342976
internlm2_scores,0.028134007892235472
eurus_prm_scores,0.027711374063938646
judge_qwen2.5-72b-instruct-turbo_verdicts,0.0261827086315777
urm_scores,0.020955799664278175
grm_llama32_scores,0.01987170463875931
internlm_scores,0.01931274055208656
eurus_prm2_scores,0.01877920943296986
offset_bias_scores,0.016819590193538004
skywork_gemma_scores,0.01575112515396069
armor_rm_coherence,0.015270934642022727
grm_scores,0.014795130222708614
armor_rm_correctness,0.014741353413529293
armor_rm_helpfulness,0.014068970849778806
qrm_gemma_complexity,0.013380013230454284
armor_rm_score,0.01292230586526343
qrm_gemma_verbosity,0.012316756590800688
grm_gemma_scores,0.011488518568891593
armor_rm_verbosity,0.011155563181155157
qrm_gemma_scores,0.011124576973486398
qrm_gemma_helpfulness,0.010716418686515213
qrm_gemma_correctness,0.010603682811209816
gpm_scores,0.010385608091897005
armor_rm_complexity,0.010246533547755916
qrm_gemma_coherence,0.010137584419329458
judge_qwq-32b-preview_verdicts,0.00787368449159209
judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts,0.0009177512516757281
