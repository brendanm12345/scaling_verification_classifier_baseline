feature,importance
qwen25_math_scores,0.14788629462190717
judge_meta-llama-3.1-405b-instruct-turbo_verdicts,0.10900871842019588
judge_gpt-4o_verdicts,0.09685325773422349
judge_llama-3.1-nemotron-70b-instruct-hf_verdicts,0.08754062998123094
judge_llama-3.3-70b-instruct-turbo_verdicts,0.06705833351860306
judge_qwen2-72b-instruct_verdicts,0.050523857076286295
skyworks_scores,0.0417047446621861
qrm_scores,0.030149387529102702
internlm2_scores,0.028036772664582127
judge_qwen2.5-72b-instruct-turbo_verdicts,0.024378605366658906
eurus_prm_scores,0.024101785849313457
internlm_scores,0.019785504707366532
urm_scores,0.01932661134271431
offset_bias_scores,0.018914094493111034
eurus_prm2_scores,0.018626636031469863
armor_rm_score,0.01617501190083863
armor_rm_coherence,0.0154854300508021
grm_llama32_scores,0.01512324848154567
armor_rm_complexity,0.013339062208828056
grm_gemma_scores,0.012860814714595313
armor_rm_verbosity,0.012819630474479753
armor_rm_correctness,0.012629506212062805
qrm_gemma_complexity,0.012474932896512534
qrm_gemma_helpfulness,0.012038904587288205
qrm_gemma_coherence,0.011658154341455397
qrm_gemma_correctness,0.011410054128642828
qrm_gemma_verbosity,0.011374910978275128
armor_rm_helpfulness,0.011269931216521046
grm_scores,0.011024938214763897
qrm_gemma_scores,0.010423836558829346
skywork_gemma_scores,0.010409714283134832
gpm_scores,0.009454756189341653
judge_qwq-32b-preview_verdicts,0.005311788994876078
judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts,0.0008201395682549149
