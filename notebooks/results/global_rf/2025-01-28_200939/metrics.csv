selection_accuracy,selection_precision,selection_recall,selection_tp,selection_tn,selection_fp,selection_fn,generation_accuracy,feature_thresholds,selection_f1,train_percentage
0.15,0.15,0.525,21,100,119,19,0.9499427999428,"{'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.49661320447921753), 'armor_rm_helpfulness': np.float64(0.5891996324062347), 'armor_rm_complexity': np.float64(0.606234610080719), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'grm_gemma_scores': np.float64(0.7077924460172653), 'skyworks_scores': np.float64(0.5416783541440964), 'armor_rm_verbosity': np.float64(0.4819148927927017), 'offset_bias_scores': np.float64(0.6787723004817963), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.5518268346786499), 'armor_rm_score': np.float64(0.604581743478775), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'urm_scores': np.float64(0.6538334786891937), 'qrm_scores': np.float64(0.7091358602046967), 'armor_rm_coherence': np.float64(0.7464228868484497), 'judge_gpt-4o_verdicts': np.float64(0.5), 'internlm_scores': np.float64(0.6052295565605164), 'grm_scores': np.float64(0.5115202963352203), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'armor_rm_correctness': np.float64(0.6482419520616531), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5)}",0.2333333333333333,0.001
0.15714285714285714,0.15714285714285714,0.55,22,100,118,18,0.9646969696969697,"{'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.3683088347315788), 'armor_rm_helpfulness': np.float64(0.6723764538764954), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'urm_scores': np.float64(0.7775284945964813), 'offset_bias_scores': np.float64(0.7289879024028778), 'grm_gemma_scores': np.float64(0.5754612535238266), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'armor_rm_score': np.float64(0.6170426905155182), 'armor_rm_coherence': np.float64(0.6898110508918762), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'armor_rm_complexity': np.float64(0.5375000089406967), 'grm_llama32_scores': np.float64(0.6828046292066574), 'skyworks_scores': np.float64(0.6936658471822739), 'qrm_scores': np.float64(0.7587038576602936), 'armor_rm_correctness': np.float64(0.6865890771150589), 'internlm_scores': np.float64(0.6398054361343384), 'armor_rm_verbosity': np.float64(0.513208657503128), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_gpt-4o_verdicts': np.float64(0.5), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'grm_scores': np.float64(0.5473895370960236), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5)}",0.24444444444444446,0.01
0.17142857142857143,0.17142857142857143,0.6,24,100,116,16,0.9758412698412698,"{'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.33441418409347534), 'offset_bias_scores': np.float64(0.7038761377334595), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'grm_scores': np.float64(0.5225895047187805), 'grm_gemma_scores': np.float64(0.5743072926998138), 'skyworks_scores': np.float64(0.6878306865692139), 'urm_scores': np.float64(0.7249739170074463), 'armor_rm_complexity': np.float64(0.49444444477558136), 'armor_rm_verbosity': np.float64(0.515001505613327), 'grm_llama32_scores': np.float64(0.6197308599948883), 'armor_rm_coherence': np.float64(0.6693784892559052), 'qrm_scores': np.float64(0.7465680092573166), 'internlm_scores': np.float64(0.6109883785247803), 'armor_rm_correctness': np.float64(0.6526175737380981), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'armor_rm_helpfulness': np.float64(0.642857164144516), 'judge_gpt-4o_verdicts': np.float64(0.5), 'armor_rm_score': np.float64(0.6020408272743225), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5)}",0.26666666666666666,0.1
0.17857142857142858,0.17857142857142858,0.6944444444444444,25,104,115,11,0.976295918367347,"{'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.3457336574792862), 'offset_bias_scores': np.float64(0.6880718916654587), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'urm_scores': np.float64(0.7367620915174484), 'armor_rm_coherence': np.float64(0.6914533078670502), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'skyworks_scores': np.float64(0.712875708937645), 'armor_rm_complexity': np.float64(0.496104434132576), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'qrm_scores': np.float64(0.7609520256519318), 'armor_rm_correctness': np.float64(0.6919020712375641), 'judge_gpt-4o_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.6328303217887878), 'armor_rm_verbosity': np.float64(0.5050602927803993), 'grm_scores': np.float64(0.5325484573841095), 'internlm_scores': np.float64(0.6264744400978088), 'armor_rm_helpfulness': np.float64(0.6838338822126389), 'armor_rm_score': np.float64(0.6380245238542557), 'grm_gemma_scores': np.float64(0.5875756442546844), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5)}",0.2840909090909091,0.3
0.17142857142857143,0.17142857142857143,0.6857142857142857,24,105,116,11,0.9752714285714286,"{'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.33961518853902817), 'offset_bias_scores': np.float64(0.6884457767009735), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'urm_scores': np.float64(0.7328161001205444), 'grm_scores': np.float64(0.5262560844421387), 'skyworks_scores': np.float64(0.6997044533491135), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'armor_rm_score': np.float64(0.6432107388973236), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.6458065509796143), 'armor_rm_verbosity': np.float64(0.4957627058029175), 'qrm_scores': np.float64(0.761697068810463), 'grm_gemma_scores': np.float64(0.5806422978639603), 'armor_rm_coherence': np.float64(0.7022506296634674), 'internlm_scores': np.float64(0.6241925209760666), 'armor_rm_complexity': np.float64(0.486837774515152), 'armor_rm_correctness': np.float64(0.6906926333904266), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'armor_rm_helpfulness': np.float64(0.6686274707317352), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5), 'judge_gpt-4o_verdicts': np.float64(0.5), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5)}",0.2742857142857143,0.5
0.17142857142857143,0.17142857142857143,0.75,24,108,116,8,0.9756666666666667,"{'judge_qwen2.5-72b-instruct-turbo_verdicts': np.float64(0.5), 'gpm_scores': np.float64(0.3465333729982376), 'offset_bias_scores': np.float64(0.6888402104377747), 'judge_claude-3-5-sonnet-latest_verdicts': np.float64(0.5), 'urm_scores': np.float64(0.7363525032997131), 'grm_scores': np.float64(0.5259620994329453), 'skyworks_scores': np.float64(0.6975517272949219), 'judge_gpt-4o_verdicts': np.float64(0.5), 'qrm_scores': np.float64(0.7531579434871674), 'armor_rm_coherence': np.float64(0.6952959895133972), 'armor_rm_correctness': np.float64(0.7008946537971497), 'grm_gemma_scores': np.float64(0.5683718025684357), 'armor_rm_complexity': np.float64(0.49038106203079224), 'internlm_scores': np.float64(0.6111212819814682), 'judge_meta-llama-3.1-405b-instruct-turbo_verdicts': np.float64(0.5), 'grm_llama32_scores': np.float64(0.6347518265247345), 'armor_rm_helpfulness': np.float64(0.6780297011137009), 'judge_gemma-2-27b-it_verdicts': np.float64(0.5), 'armor_rm_score': np.float64(0.6400309503078461), 'armor_rm_verbosity': np.float64(0.49712643027305603), 'judge_qwen2-72b-instruct_verdicts': np.float64(0.5), 'judge_llama-3.3-70b-instruct-turbo_verdicts': np.float64(0.5), 'judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts': np.float64(0.5), 'judge_llama-3.1-nemotron-70b-instruct-hf_verdicts': np.float64(0.5), 'judge_qwq-32b-preview_verdicts': np.float64(0.5)}",0.2790697674418604,0.7
