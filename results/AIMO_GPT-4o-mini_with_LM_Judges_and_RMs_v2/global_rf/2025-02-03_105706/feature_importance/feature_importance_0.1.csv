feature,importance
qwen25_math_scores,0.13871229991786
judge_llama-3.1-nemotron-70b-instruct-hf_verdicts,0.1075961127109538
judge_gpt-4o_verdicts,0.0816484490787953
judge_meta-llama-3.1-405b-instruct-turbo_verdicts,0.07577517212228026
skyworks_scores,0.04645999128714234
judge_qwen2-72b-instruct_verdicts,0.04633159000201962
judge_llama-3.3-70b-instruct-turbo_verdicts,0.044586592621738724
qrm_scores,0.04125378002894111
internlm2_scores,0.04086347188714464
internlm_scores,0.02718879255079535
judge_qwen2.5-72b-instruct-turbo_verdicts,0.026015460635755723
urm_scores,0.025965833613966934
offset_bias_scores,0.02079212403432543
eurus_prm2_scores,0.02077981925850953
eurus_prm_scores,0.02016226910147449
grm_llama32_scores,0.01640603172408
qrm_gemma_scores,0.015978348726895093
armor_rm_verbosity,0.015505320363895328
qrm_gemma_coherence,0.01500766145984516
skywork_gemma_scores,0.014915257950126727
grm_scores,0.014627556644451005
armor_rm_coherence,0.01455395531083243
qrm_gemma_verbosity,0.014497327076824109
qrm_gemma_helpfulness,0.014182438327748684
qrm_gemma_complexity,0.014095144329001703
qrm_gemma_correctness,0.012123765791314026
grm_gemma_scores,0.01203601278213511
armor_rm_helpfulness,0.012007355592743995
armor_rm_correctness,0.01171455130437448
armor_rm_score,0.011059789259262872
armor_rm_complexity,0.010939956834003929
gpm_scores,0.010574390944410794
judge_qwq-32b-preview_verdicts,0.00427338008591067
judge_nous-hermes-2-mixtral-8x7b-dpo_verdicts,0.0013699966404408408
